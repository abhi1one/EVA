{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 3 - 1st DNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# **Import Libraries and modules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8FDs1Hx0-o0",
        "colab_type": "text"
      },
      "source": [
        "installing and Importing Keras for current solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98728414-9d3b-4e9d-824e-49e0cc1a1c98"
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-qhHUhK1Ggp",
        "colab_type": "text"
      },
      "source": [
        "Importing Numpy and Keras modules as well as mnist data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "### Load pre-shuffled MNIST data into train and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxPvMT_21Mhy",
        "colab_type": "text"
      },
      "source": [
        "Loading mnist data set in train and test variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "d57ac2de-1d4d-4072-ffb2-aba191a56e9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 9s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SMpZZpo1Sv0",
        "colab_type": "text"
      },
      "source": [
        "Ploting sample from train data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "8b06649e-9bbb-4c06-bb90-3768e7954bf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff6bdea2d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1AM8Fy81X1n",
        "colab_type": "text"
      },
      "source": [
        "Reshaping all train and test data to a uniform size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjr4ODS81eUc",
        "colab_type": "text"
      },
      "source": [
        "Regularizing train and test data for float data type and division wiht 255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDZ-v1na1okY",
        "colab_type": "text"
      },
      "source": [
        "Visualizing train out put"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "d750679c-df82-4935-f396-f87306ce0426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCLqRA3o1wFq",
        "colab_type": "text"
      },
      "source": [
        "Convert 1-dimensional class arrays to 10-dimensional class matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXmnovfn1yew",
        "colab_type": "text"
      },
      "source": [
        "Viewing tranformed train out put matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "6e09c3c4-84eb-42c5-ed45-0a3b29fd210d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2PFtTZq127m",
        "colab_type": "text"
      },
      "source": [
        "Defining CNN with drop outs; not using max pooling as it may not be very useful for these size of images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "19ce6458-d861-46c1-d18b-e0d3488acdda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu')) #input 26,26 #RF 3X3\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu')) #input 24,24 #RF 7X7\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu')) #input 22,22 #RF 9X9\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu')) #input 20,20 #RF 11X11\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu')) #input 18,18 #RF 13X13\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) #input 16,16 #RF 15X15\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) #input 14,14 #RF 17X17\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) #input 12,12 #RF 19X19\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Convolution2D(10, 1)) #input 10,10 , activation='relu'\n",
        "model.add(Convolution2D(10, 10)) #input 10,10\n",
        "\n",
        "'''\n",
        "\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) #input 26,26 #RF 3X3\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) #input 24,24 #RF 7X7\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #input 22,22 #RF 14X14\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu')) #input 11,11 #RF 16X16\n",
        "model.add(Convolution2D(10, 1, activation='relu')) #input 9,9\n",
        "model.add(Convolution2D(10, 9)) #input 9X9\n",
        "'''\n",
        "'''\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', input_shape=(28,28,1)))\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu')) #input 26,26 #RF 3X3\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #input 24,24 #RF 6X6\n",
        "model.add(Convolution2D(32, 3, 3, activation='relu')) #input 12,12 #RF 8X8\n",
        "#model.add(Convolution2D(32, 3, 3, activation='relu')) #input 11,11 #RF 10X10\n",
        "model.add(Convolution2D(10, 1, activation='relu')) #input 9,9\n",
        "model.add(Convolution2D(10, 10)) #input 9X9\n",
        "'''\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", input_shape=(28, 28, 1...)`\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\")`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\")`\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\")`\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "  app.launch_new_instance()\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\")`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1mq9MtB2GgQ",
        "colab_type": "text"
      },
      "source": [
        "Printing model summary to understand current paramaters for the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzdAYg1k9K7Z",
        "colab_type": "code",
        "outputId": "acb44f28-2613-44b8-d02e-5c5b30d501c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_12 (Conv2D)           (None, 26, 26, 8)         80        \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 24, 24, 8)         584       \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 24, 24, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_14 (Conv2D)           (None, 22, 22, 8)         584       \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 22, 22, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 20, 20, 8)         584       \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 20, 20, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 18, 18, 8)         584       \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 18, 18, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 16, 16, 8)         584       \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 16, 16, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 14, 14, 16)        1168      \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 14, 14, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 12, 12, 16)        2320      \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 12, 12, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 10, 10, 16)        2320      \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 10, 10, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 10, 10, 10)        170       \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 1, 1, 10)          10010     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 18,988\n",
            "Trainable params: 18,988\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlnhdT4u2Owz",
        "colab_type": "text"
      },
      "source": [
        "Setting model's compile environment with loss function, optimizer and matrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=1.0, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06jT2a0t2Vpj",
        "colab_type": "text"
      },
      "source": [
        "Training model for 100 epoch for 128 batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xWoKhPY9Of5",
        "colab_type": "code",
        "outputId": "50148fa7-9943-4edd-a207-fecc9951c805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3468
        }
      },
      "source": [
        "history = model.fit(X_train, Y_train, batch_size=128, nb_epoch=100, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.7609 - acc: 0.7472 - val_loss: 0.3027 - val_acc: 0.9066\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.3183 - acc: 0.9009 - val_loss: 0.1877 - val_acc: 0.9364\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.2061 - acc: 0.9351 - val_loss: 0.1032 - val_acc: 0.9671\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.1610 - acc: 0.9509 - val_loss: 0.1083 - val_acc: 0.9664\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.1363 - acc: 0.9582 - val_loss: 0.0838 - val_acc: 0.9740\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.1215 - acc: 0.9618 - val_loss: 0.0698 - val_acc: 0.9779\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.1112 - acc: 0.9655 - val_loss: 0.0740 - val_acc: 0.9756\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.1044 - acc: 0.9678 - val_loss: 0.0601 - val_acc: 0.9793\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0997 - acc: 0.9693 - val_loss: 0.0556 - val_acc: 0.9824\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0943 - acc: 0.9713 - val_loss: 0.0599 - val_acc: 0.9818\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0902 - acc: 0.9725 - val_loss: 0.0598 - val_acc: 0.9814\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0866 - acc: 0.9728 - val_loss: 0.0519 - val_acc: 0.9821\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0856 - acc: 0.9733 - val_loss: 0.0486 - val_acc: 0.9851\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0813 - acc: 0.9742 - val_loss: 0.0524 - val_acc: 0.9816\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0806 - acc: 0.9750 - val_loss: 0.0472 - val_acc: 0.9840\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0790 - acc: 0.9755 - val_loss: 0.0436 - val_acc: 0.9853\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0772 - acc: 0.9761 - val_loss: 0.0421 - val_acc: 0.9859\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0771 - acc: 0.9763 - val_loss: 0.0475 - val_acc: 0.9842\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0729 - acc: 0.9768 - val_loss: 0.0427 - val_acc: 0.9862\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0705 - acc: 0.9782 - val_loss: 0.0394 - val_acc: 0.9862\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0722 - acc: 0.9774 - val_loss: 0.0414 - val_acc: 0.9865\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0702 - acc: 0.9781 - val_loss: 0.0415 - val_acc: 0.9862\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0697 - acc: 0.9791 - val_loss: 0.0431 - val_acc: 0.9862\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0676 - acc: 0.9787 - val_loss: 0.0426 - val_acc: 0.9856\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0680 - acc: 0.9784 - val_loss: 0.0372 - val_acc: 0.9884\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0664 - acc: 0.9792 - val_loss: 0.0426 - val_acc: 0.9863\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0656 - acc: 0.9797 - val_loss: 0.0371 - val_acc: 0.9880\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0647 - acc: 0.9796 - val_loss: 0.0377 - val_acc: 0.9881\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0631 - acc: 0.9805 - val_loss: 0.0426 - val_acc: 0.9869\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0616 - acc: 0.9806 - val_loss: 0.0333 - val_acc: 0.9900\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0620 - acc: 0.9798 - val_loss: 0.0332 - val_acc: 0.9883\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0629 - acc: 0.9801 - val_loss: 0.0465 - val_acc: 0.9856\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0615 - acc: 0.9806 - val_loss: 0.0404 - val_acc: 0.9856\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0614 - acc: 0.9808 - val_loss: 0.0414 - val_acc: 0.9874\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 5s 84us/step - loss: 0.0580 - acc: 0.9819 - val_loss: 0.0429 - val_acc: 0.9864\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.0599 - acc: 0.9814 - val_loss: 0.0300 - val_acc: 0.9903\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0614 - acc: 0.9801 - val_loss: 0.0346 - val_acc: 0.9890\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0583 - acc: 0.9813 - val_loss: 0.0301 - val_acc: 0.9897\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0581 - acc: 0.9812 - val_loss: 0.0333 - val_acc: 0.9887\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0580 - acc: 0.9821 - val_loss: 0.0343 - val_acc: 0.9897\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0585 - acc: 0.9822 - val_loss: 0.0352 - val_acc: 0.9898\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0561 - acc: 0.9818 - val_loss: 0.0343 - val_acc: 0.9888\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0575 - acc: 0.9822 - val_loss: 0.0322 - val_acc: 0.9899\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0546 - acc: 0.9829 - val_loss: 0.0320 - val_acc: 0.9899\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0545 - acc: 0.9831 - val_loss: 0.0334 - val_acc: 0.9892\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0564 - acc: 0.9823 - val_loss: 0.0300 - val_acc: 0.9903\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0546 - acc: 0.9828 - val_loss: 0.0286 - val_acc: 0.9903\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0542 - acc: 0.9832 - val_loss: 0.0379 - val_acc: 0.9876\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0533 - acc: 0.9830 - val_loss: 0.0299 - val_acc: 0.9903\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0549 - acc: 0.9824 - val_loss: 0.0324 - val_acc: 0.9892\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0534 - acc: 0.9831 - val_loss: 0.0284 - val_acc: 0.9911\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0542 - acc: 0.9832 - val_loss: 0.0347 - val_acc: 0.9889\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0526 - acc: 0.9830 - val_loss: 0.0310 - val_acc: 0.9898\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0552 - acc: 0.9828 - val_loss: 0.0308 - val_acc: 0.9898\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0542 - acc: 0.9832 - val_loss: 0.0288 - val_acc: 0.9912\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0528 - acc: 0.9832 - val_loss: 0.0304 - val_acc: 0.9899\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0531 - acc: 0.9828 - val_loss: 0.0324 - val_acc: 0.9904\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0534 - acc: 0.9830 - val_loss: 0.0254 - val_acc: 0.9928\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0546 - acc: 0.9822 - val_loss: 0.0340 - val_acc: 0.9902\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0500 - acc: 0.9844 - val_loss: 0.0287 - val_acc: 0.9912\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0513 - acc: 0.9840 - val_loss: 0.0350 - val_acc: 0.9897\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0507 - acc: 0.9845 - val_loss: 0.0317 - val_acc: 0.9903\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0527 - acc: 0.9832 - val_loss: 0.0286 - val_acc: 0.9915\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0511 - acc: 0.9836 - val_loss: 0.0344 - val_acc: 0.9888\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0522 - acc: 0.9831 - val_loss: 0.0308 - val_acc: 0.9907\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0516 - acc: 0.9840 - val_loss: 0.0311 - val_acc: 0.9901\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0514 - acc: 0.9838 - val_loss: 0.0288 - val_acc: 0.9914\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 5s 75us/step - loss: 0.0530 - acc: 0.9829 - val_loss: 0.0292 - val_acc: 0.9903\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0495 - acc: 0.9841 - val_loss: 0.0328 - val_acc: 0.9901\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.0506 - acc: 0.9841 - val_loss: 0.0367 - val_acc: 0.9899\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 5s 83us/step - loss: 0.0505 - acc: 0.9838 - val_loss: 0.0327 - val_acc: 0.9900\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 5s 76us/step - loss: 0.0505 - acc: 0.9841 - val_loss: 0.0308 - val_acc: 0.9909\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0518 - acc: 0.9836 - val_loss: 0.0306 - val_acc: 0.9900\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0503 - acc: 0.9840 - val_loss: 0.0300 - val_acc: 0.9910\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0497 - acc: 0.9844 - val_loss: 0.0318 - val_acc: 0.9899\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0497 - acc: 0.9849 - val_loss: 0.0330 - val_acc: 0.9899\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0491 - acc: 0.9840 - val_loss: 0.0302 - val_acc: 0.9908\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0503 - acc: 0.9842 - val_loss: 0.0295 - val_acc: 0.9913\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0486 - acc: 0.9848 - val_loss: 0.0303 - val_acc: 0.9906\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0496 - acc: 0.9845 - val_loss: 0.0288 - val_acc: 0.9904\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0502 - acc: 0.9840 - val_loss: 0.0290 - val_acc: 0.9916\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0502 - acc: 0.9840 - val_loss: 0.0313 - val_acc: 0.9897\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0488 - acc: 0.9847 - val_loss: 0.0278 - val_acc: 0.9915\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0473 - acc: 0.9852 - val_loss: 0.0349 - val_acc: 0.9892\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0505 - acc: 0.9837 - val_loss: 0.0346 - val_acc: 0.9903\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0483 - acc: 0.9846 - val_loss: 0.0284 - val_acc: 0.9910\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0488 - acc: 0.9842 - val_loss: 0.0285 - val_acc: 0.9913\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.0496 - acc: 0.9841 - val_loss: 0.0315 - val_acc: 0.9905\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 5s 80us/step - loss: 0.0484 - acc: 0.9840 - val_loss: 0.0404 - val_acc: 0.9875\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0498 - acc: 0.9844 - val_loss: 0.0301 - val_acc: 0.9906\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0471 - acc: 0.9848 - val_loss: 0.0281 - val_acc: 0.9915\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0504 - acc: 0.9840 - val_loss: 0.0316 - val_acc: 0.9905\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0501 - acc: 0.9845 - val_loss: 0.0309 - val_acc: 0.9915\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0496 - acc: 0.9840 - val_loss: 0.0313 - val_acc: 0.9913\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0473 - acc: 0.9852 - val_loss: 0.0298 - val_acc: 0.9905\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0482 - acc: 0.9840 - val_loss: 0.0312 - val_acc: 0.9908\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.0469 - acc: 0.9851 - val_loss: 0.0261 - val_acc: 0.9923\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0488 - acc: 0.9843 - val_loss: 0.0347 - val_acc: 0.9899\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0467 - acc: 0.9852 - val_loss: 0.0297 - val_acc: 0.9912\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 4s 75us/step - loss: 0.0493 - acc: 0.9845 - val_loss: 0.0281 - val_acc: 0.9918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOCND5cw2bJm",
        "colab_type": "text"
      },
      "source": [
        "Training model for another 256 batch size for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tvptcn8dxvp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "cb9203bb-1229-468c-dbac-72815d4ffd3c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#history = model.fit(x, y, validation_split=0.25, epochs=50, batch_size=16, verbose=1)\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFnk_prLfauH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "55f61588-02b4-4790-d68b-36148baad268"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=256, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.0442 - acc: 0.9860 - val_loss: 0.0276 - val_acc: 0.9919\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.0435 - acc: 0.9864 - val_loss: 0.0266 - val_acc: 0.9918\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.0447 - acc: 0.9856 - val_loss: 0.0291 - val_acc: 0.9917\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 4s 58us/step - loss: 0.0416 - acc: 0.9869 - val_loss: 0.0262 - val_acc: 0.9921\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.0414 - acc: 0.9868 - val_loss: 0.0292 - val_acc: 0.9909\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 4s 58us/step - loss: 0.0421 - acc: 0.9871 - val_loss: 0.0291 - val_acc: 0.9914\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.0425 - acc: 0.9863 - val_loss: 0.0296 - val_acc: 0.9917\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.0439 - acc: 0.9858 - val_loss: 0.0260 - val_acc: 0.9927\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.0424 - acc: 0.9865 - val_loss: 0.0311 - val_acc: 0.9912\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.0436 - acc: 0.9863 - val_loss: 0.0305 - val_acc: 0.9910\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff6bdeb2198>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPDGNUAl2hNt",
        "colab_type": "text"
      },
      "source": [
        "Training model for another 10 epochs with batch of 256 as last vla_acc is .9910"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPJQi9Gvs-mR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "2ce22e74-5cc1-45a7-8772-dc6c6961a4a2"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=256, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            " 1792/60000 [..............................] - ETA: 5s - loss: 0.0268 - acc: 0.9911"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.0428 - acc: 0.9863 - val_loss: 0.0277 - val_acc: 0.9917\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.0423 - acc: 0.9863 - val_loss: 0.0268 - val_acc: 0.9919\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.0430 - acc: 0.9860 - val_loss: 0.0276 - val_acc: 0.9924\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.0419 - acc: 0.9864 - val_loss: 0.0280 - val_acc: 0.9907\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 4s 58us/step - loss: 0.0429 - acc: 0.9863 - val_loss: 0.0305 - val_acc: 0.9908\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.0433 - acc: 0.9863 - val_loss: 0.0337 - val_acc: 0.9897\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.0423 - acc: 0.9864 - val_loss: 0.0278 - val_acc: 0.9921\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.0410 - acc: 0.9869 - val_loss: 0.0317 - val_acc: 0.9903\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.0426 - acc: 0.9866 - val_loss: 0.0263 - val_acc: 0.9917\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.0421 - acc: 0.9867 - val_loss: 0.0256 - val_acc: 0.9925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff68cdfe358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYBky-jp2tk6",
        "colab_type": "text"
      },
      "source": [
        "Training model for another 10 epochs with increased batch size of 512; expecting improvement in val acce for .9925"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRJGCtsbtK0m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "61303b4e-d531-4904-a48b-98abab425eb8"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=512, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 4s 60us/step - loss: 0.0395 - acc: 0.9874 - val_loss: 0.0248 - val_acc: 0.9925\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0385 - acc: 0.9877 - val_loss: 0.0270 - val_acc: 0.9923\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0396 - acc: 0.9876 - val_loss: 0.0304 - val_acc: 0.9910\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0381 - acc: 0.9877 - val_loss: 0.0267 - val_acc: 0.9920\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0371 - acc: 0.9878 - val_loss: 0.0264 - val_acc: 0.9925\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0387 - acc: 0.9881 - val_loss: 0.0263 - val_acc: 0.9926\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0382 - acc: 0.9882 - val_loss: 0.0285 - val_acc: 0.9920\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0407 - acc: 0.9872 - val_loss: 0.0269 - val_acc: 0.9913\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0379 - acc: 0.9876 - val_loss: 0.0286 - val_acc: 0.9915\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0383 - acc: 0.9879 - val_loss: 0.0248 - val_acc: 0.9922\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff68cdfe518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuFPOcl829xB",
        "colab_type": "text"
      },
      "source": [
        "Val_acc still .9922; training for another 10 epochs "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETVE7fXcyhtm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "a740c9f9-586e-43ee-c010-e4105e19c5c8"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=512, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            " 2560/60000 [>.............................] - ETA: 4s - loss: 0.0410 - acc: 0.9855"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.0383 - acc: 0.9880 - val_loss: 0.0284 - val_acc: 0.9914\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0387 - acc: 0.9870 - val_loss: 0.0284 - val_acc: 0.9916\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0382 - acc: 0.9876 - val_loss: 0.0268 - val_acc: 0.9918\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0394 - acc: 0.9874 - val_loss: 0.0287 - val_acc: 0.9914\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0384 - acc: 0.9874 - val_loss: 0.0267 - val_acc: 0.9922\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0372 - acc: 0.9880 - val_loss: 0.0261 - val_acc: 0.9917\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0402 - acc: 0.9873 - val_loss: 0.0272 - val_acc: 0.9914\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0395 - acc: 0.9876 - val_loss: 0.0286 - val_acc: 0.9910\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.0388 - acc: 0.9874 - val_loss: 0.0267 - val_acc: 0.9919\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.0380 - acc: 0.9879 - val_loss: 0.0260 - val_acc: 0.9914\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff68ccbe438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8wKXEVO3Hol",
        "colab_type": "text"
      },
      "source": [
        "Training for 10 epochs to increase acc from .9914"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENI9VNPdyphH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "fd766115-87ff-4ba3-da4b-2a53e7d617ed"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=1024, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            " 2048/60000 [>.............................] - ETA: 3s - loss: 0.0295 - acc: 0.9902"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.0375 - acc: 0.9884 - val_loss: 0.0268 - val_acc: 0.9919\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0352 - acc: 0.9887 - val_loss: 0.0268 - val_acc: 0.9920\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0354 - acc: 0.9890 - val_loss: 0.0256 - val_acc: 0.9919\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0353 - acc: 0.9887 - val_loss: 0.0264 - val_acc: 0.9917\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 0.0358 - acc: 0.9884 - val_loss: 0.0279 - val_acc: 0.9916\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 0.0347 - acc: 0.9892 - val_loss: 0.0260 - val_acc: 0.9927\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 0.0366 - acc: 0.9881 - val_loss: 0.0257 - val_acc: 0.9918\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 0.0353 - acc: 0.9885 - val_loss: 0.0264 - val_acc: 0.9922\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 0.0351 - acc: 0.9884 - val_loss: 0.0272 - val_acc: 0.9916\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 3s 48us/step - loss: 0.0355 - acc: 0.9881 - val_loss: 0.0281 - val_acc: 0.9916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff6b04f40b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evf2ChFE3X0y",
        "colab_type": "text"
      },
      "source": [
        "Training with increasd batch size of 2048 to try and increase accuracy of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZAGRAypy2yG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "c3eb9385-4701-4330-e9d7-9bd496f8a7bf"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=2048, nb_epoch=10, verbose=1, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "\r 2048/60000 [>.............................] - ETA: 3s - loss: 0.0478 - acc: 0.9824"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0349 - acc: 0.9885 - val_loss: 0.0263 - val_acc: 0.9924\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 46us/step - loss: 0.0341 - acc: 0.9889 - val_loss: 0.0283 - val_acc: 0.9914\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0339 - acc: 0.9893 - val_loss: 0.0266 - val_acc: 0.9919\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0336 - acc: 0.9895 - val_loss: 0.0254 - val_acc: 0.9923\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0328 - acc: 0.9894 - val_loss: 0.0271 - val_acc: 0.9917\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0329 - acc: 0.9895 - val_loss: 0.0261 - val_acc: 0.9921\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0340 - acc: 0.9887 - val_loss: 0.0259 - val_acc: 0.9922\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0342 - acc: 0.9892 - val_loss: 0.0270 - val_acc: 0.9914\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0336 - acc: 0.9892 - val_loss: 0.0265 - val_acc: 0.9917\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 3s 47us/step - loss: 0.0343 - acc: 0.9891 - val_loss: 0.0260 - val_acc: 0.9923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7ff6b04f4e80>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFvBsunz3um0",
        "colab_type": "text"
      },
      "source": [
        "Evaluating model and storing score in a variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtsH-lLk-eLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D836-UcX30QH",
        "colab_type": "text"
      },
      "source": [
        "Printing loss and accuracy of model test done in last step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkX8JMv79q9r",
        "colab_type": "code",
        "outputId": "91e19fce-7936-4fac-d94e-2bb29d9bf356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(score)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.025956442816008347, 0.9923]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed35zlSB37Mm",
        "colab_type": "text"
      },
      "source": [
        "Predicting out put for test inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCWoJkwE9suh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8INoWoRT4BBu",
        "colab_type": "text"
      },
      "source": [
        "Printing prediction and test out puts "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym7iCFBm9uBs",
        "colab_type": "code",
        "outputId": "7dbdbf86-fd34-468d-bc1b-09060b35b074",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "source": [
        "print(y_pred[:9])\n",
        "print(y_test[:9])"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8.83453048e-17 2.23902832e-22 5.03254502e-13 1.60512364e-13\n",
            "  4.02065522e-23 9.60435273e-19 0.00000000e+00 1.00000000e+00\n",
            "  1.39047658e-16 1.85744840e-11]\n",
            " [3.93890887e-21 7.82338257e-12 1.00000000e+00 3.59788031e-17\n",
            "  1.38093388e-18 1.37124121e-29 4.85452095e-11 2.52794327e-23\n",
            "  1.14365700e-16 1.80574017e-27]\n",
            " [4.05707407e-17 1.00000000e+00 2.24106909e-12 8.25022068e-17\n",
            "  6.03128159e-10 3.44477593e-14 1.42322758e-13 1.16682185e-11\n",
            "  3.78736241e-13 1.01929050e-17]\n",
            " [1.00000000e+00 5.48987821e-33 1.91183327e-17 4.73503497e-22\n",
            "  6.40486258e-19 6.92827582e-24 3.11879712e-15 4.00338433e-19\n",
            "  3.24701504e-19 7.47422979e-19]\n",
            " [8.47725630e-29 1.00496457e-21 2.52994851e-24 3.29754186e-28\n",
            "  1.00000000e+00 2.08647185e-28 4.26313701e-21 5.24219354e-19\n",
            "  6.29316102e-20 1.76778762e-15]\n",
            " [2.48050972e-12 1.00000000e+00 9.11990483e-10 4.10908282e-16\n",
            "  9.71266290e-10 5.04951478e-17 5.48377736e-14 4.93403363e-08\n",
            "  8.02247435e-11 1.75318283e-13]\n",
            " [9.30098297e-33 9.37158608e-15 2.06010754e-17 7.46690805e-26\n",
            "  1.00000000e+00 5.74528180e-22 3.80990414e-31 7.89448438e-18\n",
            "  2.99498648e-10 2.92296332e-14]\n",
            " [1.99232442e-29 3.98114961e-21 7.15147126e-12 3.90293657e-12\n",
            "  1.24935861e-04 7.24509255e-12 4.15747299e-25 9.05675749e-15\n",
            "  8.62060531e-14 9.99875069e-01]\n",
            " [1.45405634e-18 0.00000000e+00 4.04683561e-19 4.99908274e-26\n",
            "  3.41847459e-22 9.99995708e-01 4.25462213e-06 2.98711163e-29\n",
            "  2.37556552e-08 1.55013792e-20]]\n",
            "[7 2 1 0 4 1 4 9 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT--y98_dr2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "98fa545a-578f-458c-eedc-5f29f899cf2a"
      },
      "source": [
        "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
        "layer_dict"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'activation_21': <keras.layers.core.Activation at 0x7f58f2cb80f0>,\n",
              " 'conv2d_173': <keras.layers.convolutional.Conv2D at 0x7f58f2d7d748>,\n",
              " 'conv2d_174': <keras.layers.convolutional.Conv2D at 0x7f58f2d7d550>,\n",
              " 'conv2d_175': <keras.layers.convolutional.Conv2D at 0x7f58f2d27160>,\n",
              " 'conv2d_176': <keras.layers.convolutional.Conv2D at 0x7f58f2d270f0>,\n",
              " 'conv2d_177': <keras.layers.convolutional.Conv2D at 0x7f58f2cc8ba8>,\n",
              " 'conv2d_178': <keras.layers.convolutional.Conv2D at 0x7f58f2cf15c0>,\n",
              " 'flatten_21': <keras.layers.core.Flatten at 0x7f58f2c8bfd0>,\n",
              " 'max_pooling2d_13': <keras.layers.pooling.MaxPooling2D at 0x7f58f2cdde10>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GY4Upv4dsUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "%matplotlib inline\n",
        "# util function to convert a tensor into a valid image\n",
        "def deprocess_image(x):\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    #x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def vis_img_in_filter(img = np.array(X_train[2]).reshape((1, 28, 28, 1)).astype(np.float64), \n",
        "                      layer_name = 'conv2d_156'):  \n",
        "    layer_output = layer_dict[layer_name].output\n",
        "    img_ascs = list()\n",
        "    for filter_index in range(layer_output.shape[3]):\n",
        "        # build a loss function that maximizes the activation\n",
        "        # of the nth filter of the layer considered\n",
        "        loss = K.mean(layer_output[:, :, :, filter_index])\n",
        "\n",
        "        # compute the gradient of the input picture wrt this loss\n",
        "        grads = K.gradients(loss, model.input)[0]\n",
        "\n",
        "        # normalization trick: we normalize the gradient\n",
        "        grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n",
        "\n",
        "        # this function returns the loss and grads given the input picture\n",
        "        iterate = K.function([model.input], [loss, grads])\n",
        "\n",
        "        # step size for gradient ascent\n",
        "        step = 5.\n",
        "\n",
        "        img_asc = np.array(img)\n",
        "        # run gradient ascent for 20 steps\n",
        "        for i in range(20):\n",
        "            loss_value, grads_value = iterate([img_asc])\n",
        "            img_asc += grads_value * step\n",
        "\n",
        "        img_asc = img_asc[0]\n",
        "        img_ascs.append(deprocess_image(img_asc).reshape((28, 28)))\n",
        "        \n",
        "    if layer_output.shape[3] >= 35:\n",
        "        plot_x, plot_y = 6, 6\n",
        "    elif layer_output.shape[3] >= 23:\n",
        "        plot_x, plot_y = 4, 6\n",
        "    elif layer_output.shape[3] >= 11:\n",
        "        plot_x, plot_y = 2, 6\n",
        "    else:\n",
        "        plot_x, plot_y = 1, 2\n",
        "    fig, ax = plt.subplots(plot_x, plot_y, figsize = (12, 12))\n",
        "    ax[0, 0].imshow(img.reshape((28, 28)), cmap = 'gray')\n",
        "    ax[0, 0].set_title('Input image')\n",
        "    fig.suptitle('Input image and %s filters' % (layer_name,))\n",
        "    fig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\n",
        "    for (x, y) in [(i, j) for i in range(plot_x) for j in range(plot_y)]:\n",
        "        if x == 0 and y == 0:\n",
        "            continue\n",
        "        ax[x, y].imshow(img_ascs[x * plot_y + y - 1], cmap = 'gray')\n",
        "        ax[x, y].set_title('filter %d' % (x * plot_y + y - 1))\n",
        "\n",
        "vis_img_in_filter()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}